!================================================================
! Cryo-EM Denoising Training Program - v31 (Perceptual Loss)
!================================================================
! Simple 3-layer CNN for cryo-EM particle denoising.
!
! Input:  Noisy particle image (1 channel × 1024 × 1024)
! Output: Clean particle image (1 channel × 1024 × 1024)
!
! Architecture:
!   Conv1: 1 → 16 channels, 3×3 kernel, ReLU
!   Conv2: 16 → 16 channels, 3×3 kernel, ReLU
!   Conv3: 16 → 1 channel, 3×3 kernel
!
! Loss Functions:
!   - MSE: Pixel-wise mean squared error (default)
!   - Fourier: Frequency domain loss for spectral accuracy
!   - SSIM: Structural similarity for perceptual quality
!   - Gradient: Edge-preserving loss for sharpness
!
! Usage:
!   ./cryo_train_perceptual --stream                      # MSE only
!   ./cryo_train_perceptual --stream --fourier_weight 0.1 # 10% Fourier
!   ./cryo_train_perceptual --stream --ssim_weight 0.1    # 10% SSIM
!   ./cryo_train_perceptual --stream --grad_weight 0.05   # 5% gradient
!   ./cryo_train_perceptual --stream --ssim_weight 0.1 --grad_weight 0.05
!================================================================

program cryo_train_perceptual
    use cudafor
    use iso_c_binding
    use streaming_cryo_loader
    use conv2d_cudnn
    use fourier_loss_module
    use ssim_loss_module
    use gradient_loss_module
    implicit none

    ! cuDNN interface
    interface
        function cudnnCreate(handle) bind(c, name='cudnnCreate')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: handle
            integer(c_int) :: cudnnCreate
        end function
        function cudnnDestroy(handle) bind(c, name='cudnnDestroy')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle
            integer(c_int) :: cudnnDestroy
        end function
    end interface

    ! Training parameters
    integer, parameter :: BATCH_SIZE = 4
    integer, parameter :: IMG_SIZE = 1024
    integer, parameter :: IN_CHANNELS = 1
    integer, parameter :: HIDDEN_CHANNELS = 16
    integer, parameter :: OUT_CHANNELS = 1
    integer, parameter :: KERNEL_SIZE = 3
    integer, parameter :: PADDING = 1  ! Same padding

    integer :: num_epochs = 5
    real(4) :: learning_rate = 0.001
    real(4) :: fourier_weight = 0.0    ! 0 = disabled
    real(4) :: ssim_weight = 0.0       ! 0 = disabled
    real(4) :: grad_weight = 0.0       ! 0 = disabled
    integer :: max_batches = -1  ! -1 means all batches
    logical :: save_checkpoints = .false.
    character(len=256) :: checkpoint_dir = "saved_models/cryo_cnn_perceptual/"
    real(4) :: val_split = 0.1  ! 10% for validation

    ! cuDNN handle
    type(c_ptr) :: cudnn_handle

    ! CNN layers
    type(conv2d_layer_t) :: conv1, conv2, conv3

    ! Data buffers - flat format from loader
    real(4), managed, allocatable :: batch_noisy_flat(:,:)   ! (1024*1024, batch)
    real(4), managed, allocatable :: batch_clean_flat(:,:)   ! (1024*1024, batch)

    ! Data buffers - 4D format for CNN (NCHW format)
    real(4), device, allocatable :: batch_input(:,:,:,:)   ! (W, H, C, N)
    real(4), device, allocatable :: batch_target(:,:,:,:)  ! (W, H, C, N)

    ! Hidden activations
    real(4), device, allocatable :: hidden1(:,:,:,:)       ! (batch, 16, 1024, 1024)
    real(4), device, allocatable :: hidden2(:,:,:,:)       ! (batch, 16, 1024, 1024)
    real(4), device, allocatable :: output(:,:,:,:)        ! (batch, 1, 1024, 1024)

    ! Gradients
    real(4), device, allocatable :: grad_output(:,:,:,:)   ! (batch, 1, 1024, 1024)
    real(4), device, allocatable :: grad_hidden2(:,:,:,:)  ! (batch, 16, 1024, 1024)
    real(4), device, allocatable :: grad_hidden1(:,:,:,:)  ! (batch, 16, 1024, 1024)

    ! Flattened buffers for Fourier loss (batch, W*H)
    real(4), device, allocatable :: output_flat(:,:)
    real(4), device, allocatable :: target_flat(:,:)

    ! Training state
    integer :: epoch, batch_num, timestep
    integer :: actual_batch_size, total_batches, train_batches, val_batches
    real(4) :: batch_loss, mse_loss_val, fourier_loss_val, ssim_loss_val, grad_loss_val
    real(4) :: epoch_loss, avg_loss
    real(4) :: epoch_mse_loss, epoch_fourier_loss, epoch_ssim_loss, epoch_grad_loss
    real(4) :: val_loss, val_avg_loss
    real(4) :: val_mse_loss, val_fourier_loss, val_ssim_loss, val_grad_loss
    real(4) :: val_avg_mse, val_avg_fourier, val_avg_ssim, val_avg_grad
    real(4) :: epoch_start_time, epoch_end_time
    integer :: samples_processed, val_samples_processed
    integer :: istat
    real(4) :: best_val_loss
    real(4) :: mse_weight

    ! Parse command line args
    call parse_training_args()

    ! Calculate MSE weight (remaining after other losses)
    mse_weight = 1.0 - fourier_weight - ssim_weight - grad_weight
    if (mse_weight < 0.0) then
        print *, "ERROR: Total weight of perceptual losses exceeds 1.0"
        stop 1
    endif

    print *, ""
    print *, "=============================================="
    print *, "  Cryo-EM CNN Training - v31 (Perceptual Loss)"
    print *, "=============================================="
    print '(A,I8)', "  Epochs:         ", num_epochs
    print '(A,F10.6)', "  Learning rate:  ", learning_rate
    print '(A,F10.4)', "  MSE weight:     ", mse_weight
    if (fourier_weight > 0.0) print '(A,F10.4)', "  Fourier weight: ", fourier_weight
    if (ssim_weight > 0.0) print '(A,F10.4)', "  SSIM weight:    ", ssim_weight
    if (grad_weight > 0.0) print '(A,F10.4)', "  Gradient weight:", grad_weight
    print '(A,I8)', "  Batch size:     ", BATCH_SIZE
    print '(A,I8)', "  Image size:     ", IMG_SIZE
    print '(A,F6.1,A)', "  Val split:      ", val_split * 100, "%"
    if (save_checkpoints) then
        print '(A,A)', "  Checkpoints:    ", trim(checkpoint_dir)
    else
        print *, "  Checkpoints:    disabled (use --save to enable)"
    endif

    ! Initialize cuDNN
    istat = cudnnCreate(cudnn_handle)
    if (istat /= 0) then
        print *, "ERROR: Failed to create cuDNN handle"
        stop 1
    endif
    print *, "  cuDNN initialized"

    ! Initialize loss modules as needed
    if (fourier_weight > 0.0) then
        call init_fourier_loss(IMG_SIZE, IMG_SIZE)
        print *, "  Fourier loss initialized"
    endif

    if (ssim_weight > 0.0) then
        call init_ssim_loss(IMG_SIZE, IMG_SIZE, BATCH_SIZE)
        print *, "  SSIM loss initialized"
    endif

    if (grad_weight > 0.0) then
        call init_gradient_loss(IMG_SIZE, IMG_SIZE, BATCH_SIZE)
        print *, "  Gradient loss initialized"
    endif

    ! Initialize streaming data loader
    call cryo_loader_init('../data/cryo_data_streaming/train_input.bin', &
                          '../data/cryo_data_streaming/train_target.bin', &
                          10309, BATCH_SIZE)  ! Full dataset: 10,309 patches
    print *, "  Streaming data loader initialized"

    ! Initialize CNN layers
    print *, "  Initializing CNN layers..."

    ! Layer 1: 1 → 16 channels
    call conv2d_init(conv1, cudnn_handle, IN_CHANNELS, HIDDEN_CHANNELS, KERNEL_SIZE, &
                     PADDING, 1, BATCH_SIZE, IMG_SIZE, IMG_SIZE, .true.)

    ! Layer 2: 16 → 16 channels
    call conv2d_init(conv2, cudnn_handle, HIDDEN_CHANNELS, HIDDEN_CHANNELS, KERNEL_SIZE, &
                     PADDING, 1, BATCH_SIZE, IMG_SIZE, IMG_SIZE, .true.)

    ! Layer 3: 16 → 1 channel (output, no ReLU)
    call conv2d_init(conv3, cudnn_handle, HIDDEN_CHANNELS, OUT_CHANNELS, KERNEL_SIZE, &
                     PADDING, 1, BATCH_SIZE, IMG_SIZE, IMG_SIZE, .false.)

    print *, "  CNN layers initialized"

    ! Allocate buffers
    allocate(batch_noisy_flat(IMG_SIZE * IMG_SIZE, BATCH_SIZE))
    allocate(batch_clean_flat(IMG_SIZE * IMG_SIZE, BATCH_SIZE))
    allocate(batch_input(IMG_SIZE, IMG_SIZE, IN_CHANNELS, BATCH_SIZE))
    allocate(batch_target(IMG_SIZE, IMG_SIZE, OUT_CHANNELS, BATCH_SIZE))
    allocate(hidden1(IMG_SIZE, IMG_SIZE, HIDDEN_CHANNELS, BATCH_SIZE))
    allocate(hidden2(IMG_SIZE, IMG_SIZE, HIDDEN_CHANNELS, BATCH_SIZE))
    allocate(output(IMG_SIZE, IMG_SIZE, OUT_CHANNELS, BATCH_SIZE))
    allocate(grad_output(IMG_SIZE, IMG_SIZE, OUT_CHANNELS, BATCH_SIZE))
    allocate(grad_hidden2(IMG_SIZE, IMG_SIZE, HIDDEN_CHANNELS, BATCH_SIZE))
    allocate(grad_hidden1(IMG_SIZE, IMG_SIZE, HIDDEN_CHANNELS, BATCH_SIZE))

    ! Allocate flat buffers for Fourier loss
    if (fourier_weight > 0.0) then
        allocate(output_flat(IMG_SIZE * IMG_SIZE, BATCH_SIZE))
        allocate(target_flat(IMG_SIZE * IMG_SIZE, BATCH_SIZE))
    endif

    total_batches = cryo_loader_get_num_batches()
    timestep = 0

    ! Split into train/val
    val_batches = int(total_batches * val_split)
    train_batches = total_batches - val_batches
    best_val_loss = 1.0e10

    print *, ""
    print *, "=============================================="
    print *, "  Starting Training"
    print *, "=============================================="
    print '(A,I8)', "  Total batches/epoch: ", total_batches
    print '(A,I8)', "  Train batches:       ", train_batches
    print '(A,I8)', "  Val batches:         ", val_batches
    print *, ""

    ! Training loop
    do epoch = 1, num_epochs
        call cpu_time(epoch_start_time)

        call cryo_loader_start_epoch()
        epoch_loss = 0.0
        epoch_mse_loss = 0.0
        epoch_fourier_loss = 0.0
        epoch_ssim_loss = 0.0
        epoch_grad_loss = 0.0
        samples_processed = 0
        batch_num = 0

        ! === TRAINING PHASE ===
        do
            ! Get next batch (flat format from loader)
            call cryo_loader_get_batch(batch_noisy_flat, batch_clean_flat, actual_batch_size)
            if (actual_batch_size <= 0) exit

            ! Reshape flat -> 4D (W, H, C, N)
            call reshape_flat_to_4d(batch_noisy_flat, batch_input, actual_batch_size)
            call reshape_flat_to_4d(batch_clean_flat, batch_target, actual_batch_size)

            batch_num = batch_num + 1
            timestep = timestep + 1

            ! Early exit for testing
            if (max_batches > 0 .and. batch_num > max_batches) exit

            ! Stop before validation batches
            if (batch_num > train_batches) exit

            ! Forward pass: input → hidden1 → hidden2 → output
            call conv2d_forward(conv1, batch_input, hidden1)
            call relu_forward(hidden1, actual_batch_size, HIDDEN_CHANNELS, IMG_SIZE, IMG_SIZE)

            call conv2d_forward(conv2, hidden1, hidden2)
            call relu_forward(hidden2, actual_batch_size, HIDDEN_CHANNELS, IMG_SIZE, IMG_SIZE)

            call conv2d_forward(conv3, hidden2, output)

            ! Compute MSE loss and gradient
            call compute_mse_loss(output, batch_target, grad_output, actual_batch_size, mse_loss_val)

            ! Initialize combined loss with MSE
            batch_loss = mse_weight * mse_loss_val

            ! Compute Fourier loss if enabled
            if (fourier_weight > 0.0) then
                call reshape_4d_to_flat(output, output_flat, actual_batch_size)
                call reshape_4d_to_flat(batch_target, target_flat, actual_batch_size)
                call compute_fourier_loss(output_flat, target_flat, actual_batch_size, fourier_loss_val)
                batch_loss = batch_loss + fourier_weight * fourier_loss_val
                epoch_fourier_loss = epoch_fourier_loss + fourier_loss_val
            endif

            ! Compute SSIM loss if enabled
            if (ssim_weight > 0.0) then
                call compute_ssim_loss(output, batch_target, actual_batch_size, ssim_loss_val)
                batch_loss = batch_loss + ssim_weight * ssim_loss_val
                epoch_ssim_loss = epoch_ssim_loss + ssim_loss_val
            endif

            ! Compute Gradient loss if enabled
            if (grad_weight > 0.0) then
                call compute_gradient_loss(output, batch_target, actual_batch_size, grad_loss_val)
                batch_loss = batch_loss + grad_weight * grad_loss_val
                epoch_grad_loss = epoch_grad_loss + grad_loss_val
            endif

            ! Scale MSE gradient by mse_weight
            ! Note: Perceptual losses act as regularization (no gradient backprop)
            if (mse_weight < 1.0) then
                call scale_gradient(grad_output, mse_weight, actual_batch_size)
            endif

            epoch_mse_loss = epoch_mse_loss + mse_loss_val

            ! Backward pass: output ← hidden2 ← hidden1 ← input
            call conv2d_backward(conv3, hidden2, output, grad_output, grad_hidden2)
            call relu_backward(hidden2, grad_hidden2, actual_batch_size, HIDDEN_CHANNELS, IMG_SIZE, IMG_SIZE)

            call conv2d_backward(conv2, hidden1, hidden2, grad_hidden2, grad_hidden1)
            call relu_backward(hidden1, grad_hidden1, actual_batch_size, HIDDEN_CHANNELS, IMG_SIZE, IMG_SIZE)

            call conv2d_backward(conv1, batch_input, hidden1, grad_hidden1, grad_output)  ! Reuse grad_output buffer

            ! Update weights
            call conv2d_update(conv1, learning_rate, timestep)
            call conv2d_update(conv2, learning_rate, timestep)
            call conv2d_update(conv3, learning_rate, timestep)

            epoch_loss = epoch_loss + batch_loss
            samples_processed = samples_processed + 1

            ! Progress update every 1000 batches (plus first 5 for verification)
            if (mod(batch_num, 1000) == 0 .or. batch_num <= 5) then
                call print_progress(epoch, batch_num, train_batches, samples_processed, &
                                   epoch_loss, epoch_mse_loss, epoch_fourier_loss, &
                                   epoch_ssim_loss, epoch_grad_loss)
            endif
        end do

        print '(A,I5,A)', "  Processed ", samples_processed, " training batches"
        avg_loss = epoch_loss / samples_processed

        ! === VALIDATION PHASE ===
        val_loss = 0.0
        val_mse_loss = 0.0
        val_fourier_loss = 0.0
        val_ssim_loss = 0.0
        val_grad_loss = 0.0
        val_samples_processed = 0

        if (max_batches > 0) then
            ! Quick test mode - skip validation
            val_avg_loss = avg_loss
            val_avg_mse = epoch_mse_loss / samples_processed
            val_avg_fourier = epoch_fourier_loss / samples_processed
            val_avg_ssim = epoch_ssim_loss / samples_processed
            val_avg_grad = epoch_grad_loss / samples_processed
            goto 100
        endif

        do while (batch_num <= total_batches)
            call cryo_loader_get_batch(batch_noisy_flat, batch_clean_flat, actual_batch_size)
            if (actual_batch_size <= 0) exit

            ! Reshape flat -> 4D
            call reshape_flat_to_4d(batch_noisy_flat, batch_input, actual_batch_size)
            call reshape_flat_to_4d(batch_clean_flat, batch_target, actual_batch_size)

            batch_num = batch_num + 1

            ! Forward pass only (no backward)
            call conv2d_forward(conv1, batch_input, hidden1)
            call relu_forward(hidden1, actual_batch_size, HIDDEN_CHANNELS, IMG_SIZE, IMG_SIZE)

            call conv2d_forward(conv2, hidden1, hidden2)
            call relu_forward(hidden2, actual_batch_size, HIDDEN_CHANNELS, IMG_SIZE, IMG_SIZE)

            call conv2d_forward(conv3, hidden2, output)

            ! Compute all losses for validation
            call compute_mse_loss(output, batch_target, grad_output, actual_batch_size, mse_loss_val)
            val_mse_loss = val_mse_loss + mse_loss_val
            batch_loss = mse_weight * mse_loss_val

            if (fourier_weight > 0.0) then
                call reshape_4d_to_flat(output, output_flat, actual_batch_size)
                call reshape_4d_to_flat(batch_target, target_flat, actual_batch_size)
                call compute_fourier_loss(output_flat, target_flat, actual_batch_size, fourier_loss_val)
                val_fourier_loss = val_fourier_loss + fourier_loss_val
                batch_loss = batch_loss + fourier_weight * fourier_loss_val
            endif

            if (ssim_weight > 0.0) then
                call compute_ssim_loss(output, batch_target, actual_batch_size, ssim_loss_val)
                val_ssim_loss = val_ssim_loss + ssim_loss_val
                batch_loss = batch_loss + ssim_weight * ssim_loss_val
            endif

            if (grad_weight > 0.0) then
                call compute_gradient_loss(output, batch_target, actual_batch_size, grad_loss_val)
                val_grad_loss = val_grad_loss + grad_loss_val
                batch_loss = batch_loss + grad_weight * grad_loss_val
            endif

            val_loss = val_loss + batch_loss
            val_samples_processed = val_samples_processed + 1
        end do

        if (val_samples_processed > 0) then
            val_avg_loss = val_loss / val_samples_processed
            val_avg_mse = val_mse_loss / val_samples_processed
            val_avg_fourier = val_fourier_loss / val_samples_processed
            val_avg_ssim = val_ssim_loss / val_samples_processed
            val_avg_grad = val_grad_loss / val_samples_processed
        else
            val_avg_loss = 0.0
            val_avg_mse = 0.0
            val_avg_fourier = 0.0
            val_avg_ssim = 0.0
            val_avg_grad = 0.0
        endif

        call cpu_time(epoch_end_time)

100     continue

        ! Epoch summary
        print *, ""
        print '(A,I2,A)', "  ===== Epoch ", epoch, " Complete ====="
        print '(A,F10.6)', "  Train Loss: ", avg_loss
        print '(A,F10.6)', "  Train MSE:  ", epoch_mse_loss / samples_processed
        if (fourier_weight > 0.0) print '(A,F10.6)', "  Train FFT:  ", epoch_fourier_loss / samples_processed
        if (ssim_weight > 0.0) print '(A,F10.6)', "  Train SSIM: ", epoch_ssim_loss / samples_processed
        if (grad_weight > 0.0) print '(A,F10.6)', "  Train Grad: ", epoch_grad_loss / samples_processed

        if (val_samples_processed > 0) then
            print '(A,F10.6)', "  Val Loss:   ", val_avg_loss
            print '(A,F10.6)', "  Val MSE:    ", val_avg_mse
            if (fourier_weight > 0.0) print '(A,F10.6)', "  Val FFT:    ", val_avg_fourier
            if (ssim_weight > 0.0) print '(A,F10.6)', "  Val SSIM:   ", val_avg_ssim
            if (grad_weight > 0.0) print '(A,F10.6)', "  Val Grad:   ", val_avg_grad
        endif
        print '(A,F8.2,A)', "  Time:       ", epoch_end_time - epoch_start_time, " seconds"
        print '(A,F8.2,A)', "  Throughput: ", samples_processed / (epoch_end_time - epoch_start_time), " samples/sec"

        ! Save checkpoint if enabled
        if (save_checkpoints) then
            if (val_avg_loss < best_val_loss) then
                best_val_loss = val_avg_loss
                print *, "  New best model! Saving checkpoint..."
                call save_checkpoint(epoch, val_avg_loss)
            else
                print '(A,F10.6)', "  Best val loss: ", best_val_loss
            endif
        endif
        print *, ""

    end do

    print *, "=============================================="
    print *, "  Training Complete"
    print *, "=============================================="
    print '(A,F10.6)', "  Best Val Loss: ", best_val_loss
    print *, ""

    ! Cleanup
    deallocate(batch_noisy_flat, batch_clean_flat)
    deallocate(batch_input, batch_target)
    deallocate(hidden1, hidden2, output)
    deallocate(grad_output, grad_hidden2, grad_hidden1)
    if (fourier_weight > 0.0) then
        deallocate(output_flat, target_flat)
        call destroy_fourier_loss()
    endif
    if (ssim_weight > 0.0) call destroy_ssim_loss()
    if (grad_weight > 0.0) call destroy_gradient_loss()
    call conv2d_cleanup(conv1)
    call conv2d_cleanup(conv2)
    call conv2d_cleanup(conv3)
    call cryo_loader_cleanup()
    istat = cudnnDestroy(cudnn_handle)

contains

    !================================================================
    ! Print training progress
    !================================================================
    subroutine print_progress(ep, bn, tb, sp, el, em, ef, es, eg)
        integer, intent(in) :: ep, bn, tb, sp
        real(4), intent(in) :: el, em, ef, es, eg

        character(len=256) :: fmt_str

        if (fourier_weight > 0.0 .or. ssim_weight > 0.0 .or. grad_weight > 0.0) then
            write(*, '(A,I2,A,I5,A,I5,A,F10.6,A,F10.6)', advance='no') &
                "  Epoch ", ep, " | Batch ", bn, "/", tb, &
                " | Loss: ", el / sp, " | MSE: ", em / sp
            if (fourier_weight > 0.0) write(*, '(A,F8.4)', advance='no') " | FFT: ", ef / sp
            if (ssim_weight > 0.0) write(*, '(A,F8.4)', advance='no') " | SSIM: ", es / sp
            if (grad_weight > 0.0) write(*, '(A,F8.4)', advance='no') " | Grad: ", eg / sp
            write(*, *)
        else
            print '(A,I2,A,I5,A,I5,A,F10.6,A,F8.4)', &
                "  Epoch ", ep, " | Batch ", bn, "/", tb, &
                " | Loss: ", el / sp, " | RMSE: ", sqrt(em / sp)
        endif

    end subroutine print_progress

    !================================================================
    ! Reshape flat array to 4D tensor
    !================================================================
    subroutine reshape_flat_to_4d(flat, tensor_4d, batch_size)
        real(4), managed, intent(in) :: flat(:,:)
        real(4), device, intent(out) :: tensor_4d(:,:,:,:)
        integer, intent(in) :: batch_size

        integer :: b, idx, w, h

        !$cuf kernel do(2) <<< *, * >>>
        do b = 1, batch_size
            do idx = 1, IMG_SIZE * IMG_SIZE
                h = (idx - 1) / IMG_SIZE + 1
                w = mod(idx - 1, IMG_SIZE) + 1
                tensor_4d(w, h, 1, b) = flat(idx, b)
            end do
        end do

    end subroutine reshape_flat_to_4d

    !================================================================
    ! Reshape 4D tensor to flat array
    !================================================================
    subroutine reshape_4d_to_flat(tensor_4d, flat, batch_size)
        real(4), device, intent(in) :: tensor_4d(:,:,:,:)
        real(4), device, intent(out) :: flat(:,:)
        integer, intent(in) :: batch_size

        integer :: b, idx, w, h

        !$cuf kernel do(2) <<< *, * >>>
        do b = 1, batch_size
            do idx = 1, IMG_SIZE * IMG_SIZE
                h = (idx - 1) / IMG_SIZE + 1
                w = mod(idx - 1, IMG_SIZE) + 1
                flat(idx, b) = tensor_4d(w, h, 1, b)
            end do
        end do

    end subroutine reshape_4d_to_flat

    !================================================================
    ! Scale gradient by a factor
    !================================================================
    subroutine scale_gradient(grad, factor, batch_size)
        real(4), device, intent(inout) :: grad(:,:,:,:)
        real(4), intent(in) :: factor
        integer, intent(in) :: batch_size

        integer :: b, c, h, w
        integer :: wd, ht, ch

        wd = size(grad, 1)
        ht = size(grad, 2)
        ch = size(grad, 3)

        !$cuf kernel do(4) <<< *, * >>>
        do b = 1, batch_size
            do c = 1, ch
                do h = 1, ht
                    do w = 1, wd
                        grad(w, h, c, b) = grad(w, h, c, b) * factor
                    end do
                end do
            end do
        end do

    end subroutine scale_gradient

    !================================================================
    ! ReLU activation (in-place)
    !================================================================
    subroutine relu_forward(x, batch_size, channels, height, width)
        real(4), device, intent(inout) :: x(:,:,:,:)
        integer, intent(in) :: batch_size, channels, height, width
        integer :: b, c, h, w

        !$cuf kernel do(4) <<< *, * >>>
        do b = 1, batch_size
            do c = 1, channels
                do h = 1, height
                    do w = 1, width
                        if (x(w, h, c, b) < 0.0) then
                            x(w, h, c, b) = 0.0
                        endif
                    end do
                end do
            end do
        end do
    end subroutine relu_forward

    !================================================================
    ! ReLU gradient (in-place)
    !================================================================
    subroutine relu_backward(activation, grad, batch_size, channels, height, width)
        real(4), device, intent(in) :: activation(:,:,:,:)
        real(4), device, intent(inout) :: grad(:,:,:,:)
        integer, intent(in) :: batch_size, channels, height, width
        integer :: b, c, h, w

        !$cuf kernel do(4) <<< *, * >>>
        do b = 1, batch_size
            do c = 1, channels
                do h = 1, height
                    do w = 1, width
                        if (activation(w, h, c, b) <= 0.0) then
                            grad(w, h, c, b) = 0.0
                        endif
                    end do
                end do
            end do
        end do
    end subroutine relu_backward

    !================================================================
    ! MSE Loss computation
    !================================================================
    subroutine compute_mse_loss(pred, target, grad, batch_size, loss)
        real(4), device, intent(in) :: pred(:,:,:,:)
        real(4), device, intent(in) :: target(:,:,:,:)
        real(4), device, intent(out) :: grad(:,:,:,:)
        integer, intent(in) :: batch_size
        real(4), intent(out) :: loss

        real(4) :: scale
        integer :: b, c, h, w
        integer :: total_elements, wd, ht, ch
        real(4), allocatable :: h_pred(:,:,:,:), h_tgt(:,:,:,:)

        wd = size(pred, 1)
        ht = size(pred, 2)
        ch = size(pred, 3)
        total_elements = batch_size * ch * ht * wd
        scale = 2.0 / real(total_elements)

        ! Compute gradient on GPU
        !$cuf kernel do(4) <<< *, * >>>
        do b = 1, batch_size
            do c = 1, ch
                do h = 1, ht
                    do w = 1, wd
                        grad(w, h, c, b) = scale * (pred(w, h, c, b) - target(w, h, c, b))
                    end do
                end do
            end do
        end do

        ! Compute loss on host
        allocate(h_pred(wd, ht, ch, batch_size))
        allocate(h_tgt(wd, ht, ch, batch_size))
        h_pred = pred(:, :, :, 1:batch_size)
        h_tgt = target(:, :, :, 1:batch_size)
        istat = cudaDeviceSynchronize()
        loss = sum((h_pred - h_tgt)**2) / real(total_elements)

        deallocate(h_pred, h_tgt)

    end subroutine compute_mse_loss

    !================================================================
    ! Save checkpoint
    !================================================================
    subroutine save_checkpoint(epoch_num, val_loss_value)
        integer, intent(in) :: epoch_num
        real(4), intent(in) :: val_loss_value
        character(len=256) :: epoch_dir, filename
        integer :: unit_num

        write(epoch_dir, '(A,A,I4.4,A)') trim(checkpoint_dir), 'epoch_', epoch_num, '/'
        call system("mkdir -p " // trim(epoch_dir))

        write(filename, '(A,A)') trim(epoch_dir), 'loss.txt'
        open(newunit=unit_num, file=trim(filename), status='replace')
        write(unit_num, '(F12.8)') val_loss_value
        close(unit_num)

        call save_layer_weights(conv1, trim(epoch_dir) // 'conv1_')
        call save_layer_weights(conv2, trim(epoch_dir) // 'conv2_')
        call save_layer_weights(conv3, trim(epoch_dir) // 'conv3_')

    end subroutine save_checkpoint

    !================================================================
    ! Save layer weights to disk
    !================================================================
    subroutine save_layer_weights(layer, prefix)
        type(conv2d_layer_t), intent(in) :: layer
        character(len=*), intent(in) :: prefix
        real(4), allocatable :: h_weights(:,:,:,:), h_bias(:)
        integer :: unit_num

        allocate(h_weights(layer%out_channels, layer%in_channels, layer%kernel_size, layer%kernel_size))
        allocate(h_bias(layer%out_channels))
        h_weights = layer%weights
        h_bias = layer%bias
        istat = cudaDeviceSynchronize()

        open(newunit=unit_num, file=trim(prefix)//'weights.bin', &
             status='replace', access='stream', form='unformatted')
        write(unit_num) h_weights
        close(unit_num)

        open(newunit=unit_num, file=trim(prefix)//'bias.bin', &
             status='replace', access='stream', form='unformatted')
        write(unit_num) h_bias
        close(unit_num)

        deallocate(h_weights, h_bias)

    end subroutine save_layer_weights

    !================================================================
    ! Parse command line arguments
    !================================================================
    subroutine parse_training_args()
        character(len=256) :: arg
        integer :: i, num_args

        num_args = command_argument_count()
        i = 1
        do while (i <= num_args)
            call get_command_argument(i, arg)

            if (trim(arg) == '--epochs') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) num_epochs
            else if (trim(arg) == '--lr') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) learning_rate
            else if (trim(arg) == '--fourier_weight') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) fourier_weight
            else if (trim(arg) == '--ssim_weight') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) ssim_weight
            else if (trim(arg) == '--grad_weight') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) grad_weight
            else if (trim(arg) == '--max_batches') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) max_batches
            else if (trim(arg) == '--save') then
                save_checkpoints = .true.
            else if (trim(arg) == '--checkpoint_dir') then
                i = i + 1
                call get_command_argument(i, arg)
                checkpoint_dir = trim(arg) // "/"
            else if (trim(arg) == '--val_split') then
                i = i + 1
                call get_command_argument(i, arg)
                read(arg, *) val_split
            else if (trim(arg) == '--stream') then
                continue
            endif

            i = i + 1
        end do

        ! Validate weights
        if (fourier_weight < 0.0 .or. fourier_weight > 1.0) then
            print *, "ERROR: --fourier_weight must be between 0.0 and 1.0"
            stop 1
        endif
        if (ssim_weight < 0.0 .or. ssim_weight > 1.0) then
            print *, "ERROR: --ssim_weight must be between 0.0 and 1.0"
            stop 1
        endif
        if (grad_weight < 0.0 .or. grad_weight > 1.0) then
            print *, "ERROR: --grad_weight must be between 0.0 and 1.0"
            stop 1
        endif

        if (save_checkpoints) then
            call system("mkdir -p " // trim(checkpoint_dir))
        endif
    end subroutine parse_training_args

end program cryo_train_perceptual
