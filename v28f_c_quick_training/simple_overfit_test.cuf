!================================================================
! Simple Overfit Test
!================================================================
! Tests if network can learn to perfectly reconstruct a single image.
! This is the simplest possible test of gradient descent.
!
! If this works: Gradients are correct
! If this fails: Something is fundamentally wrong
!================================================================
program simple_overfit_test
    use cudafor
    use conv2d_cudnn
    use iso_c_binding
    implicit none

    ! cuDNN C interface
    interface
        function cudnnCreate(handle) bind(c, name='cudnnCreate')
            import :: c_ptr, c_int
            type(c_ptr) :: handle
            integer(c_int) :: cudnnCreate
        end function cudnnCreate
    end interface

    ! Configuration
    integer, parameter :: IMAGE_SIZE = 64  ! Smaller for faster testing
    integer, parameter :: BATCH_SIZE = 1   ! Single image
    integer, parameter :: NUM_STEPS = 500
    real(4), parameter :: LEARNING_RATE = 0.001

    ! cuDNN
    type(c_ptr) :: cudnn_handle
    integer :: stat

    ! Single conv layer (simplest possible)
    type(conv2d_layer_t) :: conv

    ! Arrays
    real(4), device, allocatable :: input(:,:,:,:), output(:,:,:,:), target(:,:,:,:)
    real(4), device, allocatable :: grad_output(:,:,:,:), grad_input(:,:,:,:)
    real(4), allocatable :: h_input(:,:,:,:), h_output(:,:,:,:), h_target(:,:,:,:), h_grad(:,:,:,:)

    ! Metrics
    real(4) :: loss
    integer :: step, i, j

    print *, ""
    print *, "================================================================"
    print *, "  Simple Overfit Test"
    print *, "================================================================"
    print *, ""
    print *, "Task: Learn to copy input to output (identity function)"
    print *, "Network: Single conv layer 1→1, 3×3"
    print *, "Data: One 64×64 constant image"
    print *, ""

    ! Initialize
    stat = cudnnCreate(cudnn_handle)
    call conv2d_init(conv, cudnn_handle, 1, 1, 3, 1, 1, BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, use_relu=.false.)

    ! Allocate
    allocate(input(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(output(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(target(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(grad_output(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(grad_input(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))

    allocate(h_input(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(h_output(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(h_target(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))
    allocate(h_grad(1, IMAGE_SIZE, IMAGE_SIZE, BATCH_SIZE))

    ! Create simple data - constant image (value = 0.5)
    h_input = 0.5
    h_target = 0.5  ! Target is the same

    input = h_input
    target = h_target

    print *, "Initial loss (should be high due to random weights):"
    call conv2d_forward(conv, input, output)
    h_output = output
    loss = sum((h_output - h_target)**2) / (IMAGE_SIZE * IMAGE_SIZE)
    print '(A, F10.6)', "  Loss: ", loss
    print '(A, F10.6)', "  Output mean: ", sum(h_output) / (IMAGE_SIZE * IMAGE_SIZE)
    print *, ""

    print *, "Training..."
    print '(A6, A15)', "Step", "Loss"
    print *, "----------------------------------------------------------------"

    do step = 1, NUM_STEPS
        ! CRITICAL FIX: Zero gradients (like PyTorch's zero_grad())
        conv%grad_weights = 0.0
        conv%grad_bias = 0.0

        ! Verify weights just before forward pass (first 3 steps)
        if (step <= 3) then
            block
                real(4), allocatable :: pre_fw_w(:,:,:,:), pre_fw_b(:)
                allocate(pre_fw_w(1,1,3,3), pre_fw_b(1))
                pre_fw_w = conv%weights
                pre_fw_b = conv%bias
                print *, "BEFORE forward step", step, ": weight_mean=", sum(pre_fw_w)/size(pre_fw_w), " bias=", pre_fw_b(1)
                deallocate(pre_fw_w, pre_fw_b)
            end block
        end if

        ! Forward
        call conv2d_forward(conv, input, output)

        ! Loss & gradient
        h_output = output
        h_grad = 2.0 * (h_output - h_target) / (IMAGE_SIZE * IMAGE_SIZE * BATCH_SIZE)
        loss = sum((h_output - h_target)**2) / (IMAGE_SIZE * IMAGE_SIZE * BATCH_SIZE)

        ! Check grad_output before backward
        if (step <= 3) then
            print *, "  grad_output mean BEFORE backward:", sum(h_grad)/(IMAGE_SIZE*IMAGE_SIZE*BATCH_SIZE)
        end if

        grad_output = h_grad

        ! Backward
        call conv2d_backward(conv, input, output, grad_output, grad_input, compute_grad_input=.false.)

        ! Check if grad_output was modified
        if (step <= 3) then
            h_grad = grad_output
            print *, "  grad_output mean AFTER backward:", sum(h_grad)/(IMAGE_SIZE*IMAGE_SIZE*BATCH_SIZE)
        end if

        ! Update with plain SGD (not Adam) to debug
        block
            real(4), allocatable :: temp_w(:,:,:,:), temp_g(:,:,:,:), temp_b(:), temp_gb(:)
            allocate(temp_w(1,1,3,3), temp_g(1,1,3,3), temp_b(1), temp_gb(1))
            temp_w = conv%weights
            temp_g = conv%grad_weights
            temp_b = conv%bias
            temp_gb = conv%grad_bias

            ! Print diagnostics (first few steps)
            if (step <= 3) then
                print *, ""
                print *, "Step", step, "diagnostics:"
                print *, "  Input mean:", sum(h_input)/(IMAGE_SIZE*IMAGE_SIZE), " (should be 0.5)"
                print *, "  Output mean:", sum(h_output)/(IMAGE_SIZE*IMAGE_SIZE), " (target: 0.5)"
                print *, "  Loss:", loss
                print *, "  grad_output mean:", sum(h_grad)/(IMAGE_SIZE*IMAGE_SIZE*BATCH_SIZE)
                print *, "  Weight grad mean:", sum(temp_g)/size(temp_g)
                print *, "  Weight mean (before update):", sum(temp_w)/size(temp_w)
                print *, "  Bias grad:", temp_gb(1)
                print *, "  Bias (before update):", temp_b(1)
            end if

            temp_w = temp_w - LEARNING_RATE * temp_g
            temp_b = temp_b - LEARNING_RATE * temp_gb
            conv%weights = temp_w
            conv%bias = temp_b

            ! Verify the update actually happened (first 3 steps)
            if (step <= 3) then
                block
                    real(4), allocatable :: verify_w(:,:,:,:), verify_b(:)
                    allocate(verify_w(1,1,3,3), verify_b(1))
                    verify_w = conv%weights
                    verify_b = conv%bias
                    print *, "  Weight mean (after update):", sum(verify_w)/size(verify_w)
                    print *, "  Bias (after update):", verify_b(1)
                    print *, "  Weight change:", sum(verify_w)/size(verify_w) - sum(temp_w)/size(temp_w) + LEARNING_RATE*sum(temp_g)/size(temp_g)
                    deallocate(verify_w, verify_b)
                end block
            end if

            deallocate(temp_w, temp_g, temp_b, temp_gb)
        end block

        ! Synchronize to ensure weight updates complete before next iteration
        stat = cudaDeviceSynchronize()

        ! Print loss
        if (mod(step, 50) == 0 .or. step == 1) then
            print '(I6, ES15.6)', step, loss
        end if

        ! Early stop if converged
        if (loss < 1.0e-6) then
            print *, ""
            print '(A, I0, A)', "Converged at step ", step, "!"
            exit
        end if
    end do

    print *, "----------------------------------------------------------------"
    print *, ""

    ! Final check
    call conv2d_forward(conv, input, output)
    h_output = output
    loss = sum((h_output - h_target)**2) / (IMAGE_SIZE * IMAGE_SIZE)

    print *, "Final results:"
    print '(A, ES12.4)', "  Loss: ", loss
    print '(A, F10.6)', "  Target mean: ", 0.5
    print '(A, F10.6)', "  Output mean: ", sum(h_output) / (IMAGE_SIZE * IMAGE_SIZE)
    print *, ""

    if (loss < 0.01) then
        print *, "✓ SUCCESS: Network learned to copy constant image"
        print *, "  → Gradients are flowing correctly"
        print *, "  → Backward pass is working"
    else
        print *, "✗ FAIL: Could not learn simple constant"
        print '(A, ES12.4)', "  Final loss: ", loss
        print *, "  → Something is wrong with gradients or optimization"
    end if
    print *, ""

    call conv2d_cleanup(conv)

end program simple_overfit_test
