!================================================================
! Single Step Test - Isolate the Issue
!================================================================
! This test does exactly ONE forward, backward, update cycle
! to see if the update moves in the right direction.
!================================================================
program test_single_step
    use cudafor
    use conv2d_cudnn
    use iso_c_binding
    implicit none

    interface
        function cudnnCreate(handle) bind(c, name='cudnnCreate')
            import :: c_ptr, c_int
            type(c_ptr) :: handle
            integer(c_int) :: cudnnCreate
        end function cudnnCreate
    end interface

    integer, parameter :: IMAGE_SIZE = 64
    integer, parameter :: BATCH_SIZE = 1
    real(4), parameter :: LEARNING_RATE = 0.001

    type(c_ptr) :: cudnn_handle
    type(conv2d_layer_t) :: conv
    integer :: stat

    real(4), device, allocatable :: input(:,:,:,:), output(:,:,:,:), target(:,:,:,:)
    real(4), device, allocatable :: grad_output(:,:,:,:), grad_input(:,:,:,:)
    real(4), allocatable :: h_input(:,:,:,:), h_output(:,:,:,:), h_target(:,:,:,:), h_grad(:,:,:,:)
    real(4), allocatable :: w_before(:,:,:,:), w_after(:,:,:,:), b_before(:), b_after(:)
    real(4) :: loss_before, loss_after, output_mean_before, output_mean_after

    print *, "========================================"
    print *, "  Single Step Test"
    print *, "========================================"
    print *, ""

    ! Initialize
    stat = cudnnCreate(cudnn_handle)
    call conv2d_init(conv, cudnn_handle, 1, 1, 3, 1, 1, BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, use_relu=.false.)

    ! Allocate
    allocate(input(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(output(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(target(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(grad_output(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(grad_input(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))

    allocate(h_input(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(h_output(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(h_target(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))
    allocate(h_grad(IMAGE_SIZE, IMAGE_SIZE, 1, BATCH_SIZE))

    allocate(w_before(1,1,3,3), w_after(1,1,3,3))
    allocate(b_before(1), b_after(1))

    ! Constant input and target
    h_input = 0.5
    h_target = 0.5
    input = h_input
    target = h_target

    print *, "--- BEFORE UPDATE ---"
    
    ! Save initial weights
    w_before = conv%weights
    b_before = conv%bias
    print *, "Weight mean:", sum(w_before)/size(w_before)
    print *, "Bias:", b_before(1)

    ! Forward pass
    call conv2d_forward(conv, input, output)
    h_output = output
    output_mean_before = sum(h_output) / size(h_output)
    loss_before = sum((h_output - h_target)**2) / size(h_output)
    
    print *, "Output mean:", output_mean_before, " (target: 0.5)"
    print *, "Loss:", loss_before
    print *, ""

    ! Compute gradient
    h_grad = 2.0 * (h_output - h_target) / size(h_output)
    grad_output = h_grad
    
    print *, "Gradient info:"
    print *, "  grad_output mean:", sum(h_grad)/size(h_grad)
    print *, "  Error (output-target):", output_mean_before - 0.5
    print *, "  Expected direction: output should INCREASE (move toward 0.5)"
    print *, ""

    ! Backward pass
    conv%grad_weights = 0.0
    conv%grad_bias = 0.0
    call conv2d_backward(conv, input, output, grad_output, grad_input, compute_grad_input=.false.)

    ! Check gradients
    block
        real(4), allocatable :: g_w(:,:,:,:), g_b(:)
        allocate(g_w(1,1,3,3), g_b(1))
        g_w = conv%grad_weights
        g_b = conv%grad_bias
        print *, "Computed gradients:"
        print *, "  Weight grad mean:", sum(g_w)/size(g_w)
        print *, "  Bias grad:", g_b(1)
        print *, ""
    end block

    ! Update weights
    block
        real(4), allocatable :: w(:,:,:,:), g(:,:,:,:), b(:), gb(:)
        allocate(w(1,1,3,3), g(1,1,3,3), b(1), gb(1))
        w = conv%weights
        g = conv%grad_weights
        b = conv%bias
        gb = conv%grad_bias
        
        w = w - LEARNING_RATE * g
        b = b - LEARNING_RATE * gb
        
        conv%weights = w
        conv%bias = b
    end block

    stat = cudaDeviceSynchronize()

    ! Save updated weights
    w_after = conv%weights
    b_after = conv%bias

    print *, "--- AFTER UPDATE ---"
    print *, "Weight mean:", sum(w_after)/size(w_after)
    print *, "Bias:", b_after(1)
    print *, "Weight change:", sum(w_after - w_before)/size(w_after)
    print *, "Bias change:", b_after(1) - b_before(1)
    print *, ""

    ! Forward pass again
    call conv2d_forward(conv, input, output)
    h_output = output
    output_mean_after = sum(h_output) / size(h_output)
    loss_after = sum((h_output - h_target)**2) / size(h_output)
    
    print *, "Output mean:", output_mean_after, " (target: 0.5)"
    print *, "Loss:", loss_after
    print *, ""

    ! Analysis
    print *, "========================================"
    print *, "  ANALYSIS"
    print *, "========================================"
    print *, "Output change:", output_mean_after - output_mean_before
    print *, "Loss change:", loss_after - loss_before
    print *, ""
    
    if (loss_after < loss_before) then
        print *, "✓ SUCCESS: Loss decreased (correct direction)"
    else
        print *, "✗ FAIL: Loss increased (WRONG direction!)"
        print *, ""
        print *, "This should be impossible with correct gradients!"
    end if

    call conv2d_cleanup(conv)

end program test_single_step
