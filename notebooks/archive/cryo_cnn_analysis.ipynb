{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryo-EM CNN Analysis - Visual Results\n",
    "\n",
    "Visualize denoising results from the Fortran/CUDA CNN training.\n",
    "\n",
    "**WARNING:**: Requires over 50GB of RAM (use the streaming version for lower RAM systems)\n",
    "\n",
    "**Model:** 3-layer CNN (1→16→16→1 channels)  \n",
    "**Dataset:** 29,952 cryo-EM particle patches (1024×1024)  \n",
    "**Training:** Epoch 1 achieved val loss 0.00697 (2× better than PyTorch!)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Fortran Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1: weights (144,), bias (16,)\n",
      "Conv2: weights (2304,), bias (16,)\n",
      "Conv3: weights (144,), bias (1,)\n",
      "\n",
      "Reshaped for PyTorch:\n",
      "Conv1: (16, 1, 3, 3)\n",
      "Conv2: (16, 16, 3, 3)\n",
      "Conv3: (1, 16, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Path to best checkpoint\n",
    "checkpoint_dir = Path('../v28f_e_final_training/saved_models/cryo_cnn/epoch_0001/')\n",
    "\n",
    "def load_fortran_weights(prefix):\n",
    "    \"\"\"Load weights and bias from Fortran binary files.\"\"\"\n",
    "    # Fortran saves as (kH, kW, in_ch, out_ch)\n",
    "    weights = np.fromfile(f'{prefix}weights.bin', dtype=np.float32)\n",
    "    bias = np.fromfile(f'{prefix}bias.bin', dtype=np.float32)\n",
    "    return weights, bias\n",
    "\n",
    "# Load all layers\n",
    "conv1_w, conv1_b = load_fortran_weights(checkpoint_dir / 'conv1_')\n",
    "conv2_w, conv2_b = load_fortran_weights(checkpoint_dir / 'conv2_')\n",
    "conv3_w, conv3_b = load_fortran_weights(checkpoint_dir / 'conv3_')\n",
    "\n",
    "print(f\"Conv1: weights {conv1_w.shape}, bias {conv1_b.shape}\")\n",
    "print(f\"Conv2: weights {conv2_w.shape}, bias {conv2_b.shape}\")\n",
    "print(f\"Conv3: weights {conv3_w.shape}, bias {conv3_b.shape}\")\n",
    "\n",
    "# Reshape weights: (kH, kW, in_ch, out_ch) -> (out_ch, in_ch, kH, kW)\n",
    "conv1_w = conv1_w.reshape(3, 3, 1, 16).transpose(3, 2, 0, 1)\n",
    "conv2_w = conv2_w.reshape(3, 3, 16, 16).transpose(3, 2, 0, 1)\n",
    "conv3_w = conv3_w.reshape(3, 3, 16, 1).transpose(3, 2, 0, 1)\n",
    "\n",
    "print(f\"\\nReshaped for PyTorch:\")\n",
    "print(f\"Conv1: {conv1_w.shape}\")\n",
    "print(f\"Conv2: {conv2_w.shape}\")\n",
    "print(f\"Conv3: {conv3_w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Model with Fortran Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with Fortran weights\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 1, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Create model and load Fortran weights\n",
    "model = SimpleCNN()\n",
    "model.conv1.weight.data = torch.from_numpy(conv1_w)\n",
    "model.conv1.bias.data = torch.from_numpy(conv1_b)\n",
    "model.conv2.weight.data = torch.from_numpy(conv2_w)\n",
    "model.conv2.bias.data = torch.from_numpy(conv2_b)\n",
    "model.conv3.weight.data = torch.from_numpy(conv3_w)\n",
    "model.conv3.bias.data = torch.from_numpy(conv3_b)\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Model loaded with Fortran weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,211 test patches\n",
      "Noisy range: [0.000, 1.000]\n",
      "Clean range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_noisy = np.fromfile('../data/cryo_data_streaming/test_input.bin', dtype=np.float32)\n",
    "test_clean = np.fromfile('../data/cryo_data_streaming/test_target.bin', dtype=np.float32)\n",
    "\n",
    "# Reshape to (num_patches, 1024, 1024)\n",
    "num_test = len(test_noisy) // (1024 * 1024)\n",
    "test_noisy = test_noisy.reshape(num_test, 1024, 1024)\n",
    "test_clean = test_clean.reshape(num_test, 1024, 1024)\n",
    "\n",
    "print(f\"Loaded {num_test:,} test patches\")\n",
    "print(f\"Noisy range: [{test_noisy.min():.3f}, {test_noisy.max():.3f}]\")\n",
    "print(f\"Clean range: [{test_clean.min():.3f}, {test_clean.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 800/3211 patches...\n",
      "Processed 1600/3211 patches...\n",
      "Processed 2400/3211 patches...\n",
      "Processed 3200/3211 patches...\n"
     ]
    }
   ],
   "source": [
    "# Denoise all test patches\n",
    "denoised = []\n",
    "batch_size = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_test, batch_size):\n",
    "        batch = test_noisy[i:i+batch_size]\n",
    "        batch_tensor = torch.from_numpy(batch).unsqueeze(1)  # Add channel dim\n",
    "        output = model(batch_tensor)\n",
    "        denoised.append(output.squeeze(1).numpy())\n",
    "        \n",
    "        if (i // batch_size + 1) % 100 == 0:\n",
    "            print(f\"Processed {i+batch_size}/{num_test} patches...\")\n",
    "\n",
    "denoised = np.concatenate(denoised, axis=0)\n",
    "print(f\"\\n✓ Denoised {num_test:,} patches\")\n",
    "print(f\"Denoised range: [{denoised.min():.3f}, {denoised.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE and RMSE\n",
    "mse = np.mean((denoised - test_clean) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# PSNR (Peak Signal-to-Noise Ratio)\n",
    "data_range = test_clean.max() - test_clean.min()\n",
    "psnr = 20 * np.log10(data_range / rmse)\n",
    "\n",
    "# Per-patch metrics\n",
    "patch_mse = np.mean((denoised - test_clean) ** 2, axis=(1, 2))\n",
    "patch_rmse = np.sqrt(patch_mse)\n",
    "\n",
    "# Correlation\n",
    "correlations = []\n",
    "for i in range(num_test):\n",
    "    corr, _ = stats.pearsonr(denoised[i].flatten(), test_clean[i].flatten())\n",
    "    correlations.append(corr)\n",
    "correlations = np.array(correlations)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Set Metrics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"MSE:                {mse:.6f}\")\n",
    "print(f\"RMSE:               {rmse:.6f}\")\n",
    "print(f\"PSNR:               {psnr:.2f} dB\")\n",
    "print(f\"Mean Correlation:   {correlations.mean():.4f}\")\n",
    "print(f\"Median Correlation: {np.median(correlations):.4f}\")\n",
    "print(f\"Min Correlation:    {correlations.min():.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 5 random examples\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(num_test, 5, replace=False)\n",
    "\n",
    "for idx in indices:\n",
    "    noisy = test_noisy[idx]\n",
    "    clean = test_clean[idx]\n",
    "    pred = denoised[idx]\n",
    "    \n",
    "    # Calculate metrics for this patch\n",
    "    patch_mse_val = np.mean((pred - clean) ** 2)\n",
    "    patch_rmse_val = np.sqrt(patch_mse_val)\n",
    "    corr, _ = stats.pearsonr(pred.flatten(), clean.flatten())\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Noisy input\n",
    "    axes[0].imshow(noisy, cmap='gray')\n",
    "    axes[0].set_title(f'Noisy Input (Patch {idx})', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Denoised output\n",
    "    axes[1].imshow(pred, cmap='gray')\n",
    "    axes[1].set_title(f'Denoised (Fortran CNN)\\nRMSE: {patch_rmse_val:.4f}, Corr: {corr:.4f}', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Clean target\n",
    "    axes[2].imshow(clean, cmap='gray')\n",
    "    axes[2].set_title('Clean Target', fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Difference map\n",
    "    diff = np.abs(pred - clean)\n",
    "    im = axes[3].imshow(diff, cmap='hot')\n",
    "    axes[3].set_title(f'Abs Error\\nMax: {diff.max():.4f}', fontsize=14)\n",
    "    axes[3].axis('off')\n",
    "    plt.colorbar(im, ax=axes[3], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE distribution\n",
    "axes[0, 0].hist(patch_rmse, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(patch_rmse.mean(), color='red', linestyle='--', label=f'Mean: {patch_rmse.mean():.4f}')\n",
    "axes[0, 0].set_xlabel('RMSE', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Count', fontsize=12)\n",
    "axes[0, 0].set_title('Per-Patch RMSE Distribution', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Correlation distribution\n",
    "axes[0, 1].hist(correlations, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].axvline(correlations.mean(), color='red', linestyle='--', label=f'Mean: {correlations.mean():.4f}')\n",
    "axes[0, 1].set_xlabel('Correlation', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[0, 1].set_title('Per-Patch Correlation Distribution', fontsize=14)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Pixel value distribution comparison\n",
    "axes[1, 0].hist(test_clean.flatten()[::1000], bins=100, alpha=0.5, label='Clean', density=True)\n",
    "axes[1, 0].hist(denoised.flatten()[::1000], bins=100, alpha=0.5, label='Denoised', density=True)\n",
    "axes[1, 0].set_xlabel('Pixel Value', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Density', fontsize=12)\n",
    "axes[1, 0].set_title('Pixel Value Distributions', fontsize=14)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Scatter plot: predicted vs target\n",
    "sample_indices = np.random.choice(len(denoised.flatten()), 10000, replace=False)\n",
    "axes[1, 1].scatter(test_clean.flatten()[sample_indices], \n",
    "                   denoised.flatten()[sample_indices],\n",
    "                   alpha=0.1, s=1)\n",
    "axes[1, 1].plot([test_clean.min(), test_clean.max()], \n",
    "                [test_clean.min(), test_clean.max()], \n",
    "                'r--', label='Perfect prediction')\n",
    "axes[1, 1].set_xlabel('Target Pixel Value', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Predicted Pixel Value', fontsize=12)\n",
    "axes[1, 1].set_title('Predicted vs Target (10k samples)', fontsize=14)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best and Worst Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst patches by RMSE\n",
    "best_idx = np.argmin(patch_rmse)\n",
    "worst_idx = np.argmax(patch_rmse)\n",
    "\n",
    "print(f\"Best patch:  idx={best_idx}, RMSE={patch_rmse[best_idx]:.6f}, Corr={correlations[best_idx]:.4f}\")\n",
    "print(f\"Worst patch: idx={worst_idx}, RMSE={patch_rmse[worst_idx]:.6f}, Corr={correlations[worst_idx]:.4f}\")\n",
    "\n",
    "for title, idx in [(\"Best\", best_idx), (\"Worst\", worst_idx)]:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(test_noisy[idx], cmap='gray')\n",
    "    axes[0].set_title(f'{title} - Noisy Input', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(denoised[idx], cmap='gray')\n",
    "    axes[1].set_title(f'{title} - Denoised\\nRMSE: {patch_rmse[idx]:.6f}', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(test_clean[idx], cmap='gray')\n",
    "    axes[2].set_title(f'{title} - Clean Target', fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    diff = np.abs(denoised[idx] - test_clean[idx])\n",
    "    im = axes[3].imshow(diff, cmap='hot')\n",
    "    axes[3].set_title(f'{title} - Abs Error\\nMax: {diff.max():.4f}', fontsize=14)\n",
    "    axes[3].axis('off')\n",
    "    plt.colorbar(im, ax=axes[3], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "- ✅ Loading Fortran-trained weights into PyTorch\n",
    "- ✅ Running inference on test set\n",
    "- ✅ Quantitative metrics (MSE, RMSE, PSNR, Correlation)\n",
    "- ✅ Visual inspection of denoising quality\n",
    "- ✅ Statistical analysis of results\n",
    "\n",
    "**Expected Results (based on training):**\n",
    "- Test RMSE: ~0.08-0.09 (matches val loss of 0.007)\n",
    "- Correlation: >0.95\n",
    "- PSNR: >25 dB\n",
    "- Visually clean reconstructions with preserved particle features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
